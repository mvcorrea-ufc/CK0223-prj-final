import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from dython.nominal import associations
import warnings
from reporting import ReportGenerator
from collections import Counter
import re

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

def get_project_root() -> str:
    """Returns the absolute path to the project root."""
    return os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

def main():
    """
    Main function to perform Exploratory Data Analysis for Part 3 and generate a report.
    """
    # --- Setup ---
    project_root = get_project_root()
    processed_csv_path = os.path.join(project_root, 'prj_files', 'fakeTelegram.BR_2022_processed.csv')
    images_dir = os.path.join(project_root, 'prj_part03', 'images')
    os.makedirs(images_dir, exist_ok=True)
    
    report = ReportGenerator(
        title="Exploratory Data Analysis Report (Lista 3)",
        introduction="This report presents the findings from the EDA performed on the processed Telegram dataset, addressing all 40 questions from the assignment."
    )

    # --- Step 1: Load and Clean Data (Tasks a, b, c, d) ---
    report.add_section("Data Loading and Initial Cleaning (Tasks a, b, c, d)")
    report.add_question("a", "Load the dataset `fakeTelegram.BR_2022.csv`.")
    if not os.path.exists(processed_csv_path):
        report.add_text("Processed data file not found. Please run Part 1 first to generate `fakeTelegram.BR_2022_processed.csv`.")
        return
    
    df = pd.read_csv(processed_csv_path)
    report.add_text("Successfully loaded processed data. Initial preview:")
    report.add_table(df.head())

    report.add_question("b", "Remove 'trava-zaps'.")
    initial_rows_b = len(df)
    df = df[~df['text_content_anonymous'].str.contains('trava-zaps', na=False, case=False)]
    report.add_text(f"Removed {initial_rows_b - len(df)} rows containing 'trava-zaps'.")
    
    report.add_question("c", "Remove duplicate rows.")
    initial_rows_c = len(df)
    df.drop_duplicates(inplace=True)
    report.add_text(f"Removed {initial_rows_c - len(df)} duplicate rows.")
    
    report.add_question("d", "Remove texts with less than 5 words.")
    initial_rows_d = len(df)
    df = df[df['words'] >= 5]
    report.add_text(f"Removed {initial_rows_d - len(df)} rows with less than 5 words.")
    report.add_text(f"Data cleaned. Final shape: **{df.shape}**")

    # --- Step 2: Numerical Attribute Analysis (Task e) ---
    report.add_section("e: Numerical Attribute Analysis")
    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()
    
    for col in numerical_cols:
        report.add_section(f"Analysis of '{col}'", level=3)
        
        # e.1: Central Tendency
        central_tendency = df[col].describe()[['mean', '50%', 'count']]
        central_tendency.rename({'50%': 'median'}, inplace=True)
        report.add_question("e.1", "Measures of Central Tendency")
        report.add_table(central_tendency.to_frame().T, title="Central Tendency")

        # e.2: Variability
        variability = df[col].describe()[['std', 'min', 'max']]
        report.add_question("e.2", "Measures of Variability")
        report.add_table(variability.to_frame().T, title="Variability")

        # e.3: Frequency Table and Histogram
        report.add_question("e.3", "Frequency Table and Histogram")
        # For frequency table, show value counts for non-unique columns or a sample
        if df[col].nunique() < 50: # Arbitrary threshold for frequency table
            report.add_table(df[col].value_counts().reset_index().rename(columns={'index': col, col: 'Count'}), title="Frequency Table (Top 10)")
        else:
            report.add_text("Frequency table not generated for this column due to high cardinality.")

        plt.figure(figsize=(8, 5))
        sns.histplot(df[col], kde=True, bins=30)
        plt.title(f'Histogram of {col}')
        hist_path = os.path.join(images_dir, f'hist_{col}.png')
        plt.savefig(hist_path)
        plt.close()
        report.add_image(f"Histogram of {col}", f"./images/hist_{col}.png")

        # e.4: Boxplot
        report.add_question("e.4", "Boxplot")
        plt.figure(figsize=(8, 5))
        sns.boxplot(x=df[col])
        plt.title(f'Boxplot of {col}')
        box_path = os.path.join(images_dir, f'box_{col}.png')
        plt.savefig(box_path)
        plt.close()
        report.add_image(f"Boxplot of {col}", f"./images/box_{col}.png")

        # e.5: QQ-Plot
        report.add_question("e.5", "QQ-Plot")
        plt.figure(figsize=(8, 5))
        stats.probplot(df[col].dropna(), dist="norm", plot=plt)
        plt.title(f'QQ-Plot of {col}')
        qq_path = os.path.join(images_dir, f'qq_{col}.png')
        plt.savefig(qq_path)
        plt.close()
        report.add_image(f"QQ-Plot of {col}", f"./images/qq_{col}.png")

        # e.6: Teste de Normalidade (Shapiro-Wilk)
        report.add_question("e.6", "Test of Normality (Shapiro-Wilk)")
        if len(df[col].dropna()) > 5000: # Shapiro-Wilk is sensitive to large N
            report.add_text("Shapiro-Wilk test skipped for large dataset (N > 5000). Consider alternative tests like Kolmogorov-Smirnov or visual inspection of QQ-Plot.")
        elif len(df[col].dropna()) > 3: # Minimum for Shapiro-Wilk
            stat, p = stats.shapiro(df[col].dropna())
            report.add_text(f"Shapiro-Wilk Test: Statistic={stat:.3f}, p={p:.3f}")
            if p > 0.05:
                report.add_text("Sample looks Gaussian (fail to reject H0).")
            else:
                report.add_text("Sample does not look Gaussian (reject H0).")
        else:
            report.add_text("Not enough data points for Shapiro-Wilk test.")

        # e.7: Best Fit Distribution (Visual inspection/common distributions)
        report.add_question("e.7", "Best Fit Distribution")
        report.add_text("Identifying the 'best fit distribution' rigorously requires specialized libraries (e.g., `fitter`). For this report, we rely on visual inspection of histograms and QQ-plots, and common knowledge of data types. For example, 'sharings' might follow a Poisson or Negative Binomial distribution, while 'score_sentiment' might be multimodal.")
        
    # --- Task f: Numerical Pair Analysis ---
    report.add_section("f: Numerical Pair Analysis")
    report.add_question("f.1", "Appropriate Correlation Coefficient")
    corr_matrix = df[numerical_cols].corr(method='pearson')
    report.add_table(corr_matrix, title="Pearson Correlation Matrix")
    
    report.add_question("f.2", "Scatter Plot (Heatmap for overview)")
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm')
    plt.title('Correlation Matrix of Numerical Attributes')
    corr_heatmap_path = os.path.join(images_dir, 'correlation_heatmap.png')
    plt.savefig(corr_heatmap_path)
    plt.close()
    report.add_image("Pearson Correlation Heatmap", "./images/correlation_heatmap.png")

    # --- Task g: Categorical Pair Analysis ---
    report.add_section("g: Categorical Pair Analysis")
    report.add_question("g.1", "Cramer's V Method Result")
    categorical_cols = ['media_type', 'message_type', 'messenger']
    # Ensure columns exist and are not entirely null
    categorical_cols = [col for col in categorical_cols if col in df.columns and not df[col].isnull().all()]

    if len(categorical_cols) > 1:
        # Using a sample to speed up the calculation for large datasets
        df_sample = df[categorical_cols].sample(n=min(5000, len(df)), random_state=42)
        cramers_v_matrix = associations(df_sample, nom_nom_assoc='cramer', compute_only=True)['corr']
        report.add_table(cramers_v_matrix, title="Cramer's V Matrix for Categorical Attributes")

        plt.figure(figsize=(10, 8))
        sns.heatmap(cramers_v_matrix, annot=True, fmt=".2f", cmap='viridis')
        plt.title("Cramer's V Correlation Matrix of Categorical Attributes")
        cramers_heatmap_path = os.path.join(images_dir, 'cramers_v_heatmap.png')
        plt.savefig(cramers_heatmap_path)
        plt.close()
        report.add_image("Cramer's V Heatmap", "./images/cramers_v_heatmap.png")
    else:
        report.add_text("Not enough categorical columns with data for Cramer's V analysis.")

    # --- Task h: Visualizations (40 items) ---
    report.add_section("h: Comprehensive Visualizations")

    # h.1: Quantities of groups, users, and messages
    report.add_question("h.1", "Quantities of groups, users, and messages")
    total_groups = df['id_group_anonymous'].nunique()
    total_users = df['id_member_anonymous'].nunique()
    total_messages = len(df)
    summary_data = {'Category': ['Groups', 'Users', 'Messages'], 'Count': [total_groups, total_users, total_messages]}
    summary_df = pd.DataFrame(summary_data)
    plt.figure(figsize=(8, 5))
    sns.barplot(x='Category', y='Count', data=summary_df)
    plt.title('Total Quantities of Groups, Users, and Messages')
    summary_bar_path = os.path.join(images_dir, 'h1_summary_quantities.png')
    plt.savefig(summary_bar_path)
    plt.close()
    report.add_image("Total Quantities", f"./images/{os.path.basename(summary_bar_path)}")

    # h.2: Text vs. Media Messages
    report.add_question("h.2", "Quantity of messages with only text vs. with media")
    text_media_counts = df['has_media'].value_counts()
    text_media_counts.index = ['Text Only', 'With Media']
    plt.figure(figsize=(8, 8))
    plt.pie(text_media_counts, labels=text_media_counts.index, autopct='%1.1f%%', startangle=90)
    plt.title('Proportion of Text vs. Media Messages')
    pie_chart_path = os.path.join(images_dir, 'h2_text_vs_media_pie.png')
    plt.savefig(pie_chart_path)
    plt.close()
    report.add_image("Text vs. Media Proportion", f"./images/{os.path.basename(pie_chart_path)}")

    # h.3: Quantidade de mensagens por tipo de m√≠dia (jpg, mp4 etc)
    report.add_question("h.3", "Quantity of messages by media type (jpg, mp4 etc)")
    media_type_counts = df['media_type'].value_counts().head(10)
    if not media_type_counts.empty:
        plt.figure(figsize=(10, 6))
        sns.barplot(x=media_type_counts.values, y=media_type_counts.index)
        plt.title('Top 10 Media Types')
        plt.xlabel('Count')
        plt.ylabel('Media Type')
        media_type_path = os.path.join(images_dir, 'h3_media_type_counts.png')
        plt.savefig(media_type_path)
        plt.close()
        report.add_image("Top 10 Media Types", f"./images/{os.path.basename(media_type_path)}")
    else:
        report.add_text("No media types found to plot.")

    # h.4: A rela√ß√£o entre a quantidade de mensagens e a quantidade de palavras presente nas mensagens;
    report.add_question("h.4", "Relationship between message count and word count")
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x='words', y='caracteres', data=df.sample(n=min(5000, len(df)), random_state=42), alpha=0.5)
    plt.title('Message Length (Characters vs. Words)')
    plt.xlabel('Word Count')
    plt.ylabel('Character Count')
    word_char_scatter_path = os.path.join(images_dir, 'h4_word_char_scatter.png')
    plt.savefig(word_char_scatter_path)
    plt.close()
    report.add_image("Message Length (Characters vs. Words)", f"./images/{os.path.basename(word_char_scatter_path)}")

    # h.5-h.9: Location-based questions (Skipped as per Part 2)
    report.add_question("h.5-h.9", "Location-based analysis (State, Country, Brazil vs. Foreign)")
    report.add_text("These questions require location data (state, country) which is not present in the dataset. Skipping these questions.")

    # h.10: As 30 URLs que mais se repetem (mais compartilhadas);
    report.add_question("h.10", "Top 30 most repeated URLs")
    top_urls = df['media_url'].value_counts().head(30).reset_index()
    top_urls.columns = ['URL', 'Count']
    if not top_urls.empty:
        report.add_table(top_urls, title="Top 30 URLs")
    else:
        report.add_text("No URLs found to list.")

    # h.11: Os 30 dom√≠nios que mais se repetem (mais compartilhados);
    report.add_question("h.11", "Top 30 most repeated domains")
    df['domain'] = df['media_url'].astype(str).apply(lambda x: re.search(r'^(?:https?:\/\/)?(?:[^@\n]+@)?(?:www\.)?([^:\/\n?]+)', x).group(1) if re.search(r'^(?:https?:\/\/)?(?:[^@\n]+@)?(?:www\.)?([^:\/\n?]+)', x) else None)
    top_domains = df['domain'].value_counts().head(30).reset_index()
    top_domains.columns = ['Domain', 'Count']
    if not top_domains.empty:
        report.add_table(top_domains, title="Top 30 Domains")
    else:
        report.add_text("No domains found to list.")

    # h.12: Os 30 usu√°rios mais ativos;
    report.add_question("h.12", "Top 30 most active users")
    top_active_users = df['id_member_anonymous'].value_counts().head(30).reset_index()
    top_active_users.columns = ['User ID', 'Message Count']
    if not top_active_users.empty:
        report.add_table(top_active_users, title="Top 30 Active Users")
    else:
        report.add_text("No active users found.")

    # h.13: Rela√ß√£o entre quantidade de mensagens contendo somente texto e mensagens com tendo m√≠dia dos usu√°rios mais ativos;
    report.add_question("h.13", "Text vs. media messages for most active users")
    active_users_df = df[df['id_member_anonymous'].isin(top_active_users['User ID'])]
    active_users_media_counts = active_users_df.groupby('id_member_anonymous')['has_media'].value_counts().unstack(fill_value=0)
    active_users_media_counts.columns = ['Text Only', 'With Media']
    active_users_media_counts['Total'] = active_users_media_counts['Text Only'] + active_users_media_counts['With Media']
    active_users_media_counts = active_users_media_counts.sort_values('Total', ascending=False).head(10) # Top 10 for visualization
    
    if not active_users_media_counts.empty:
        active_users_media_counts.plot(kind='bar', stacked=True, figsize=(12, 7))
        plt.title('Text vs. Media Messages for Top Active Users')
        plt.xlabel('User ID')
        plt.ylabel('Message Count')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        active_users_media_path = os.path.join(images_dir, 'h13_active_users_media.png')
        plt.savefig(active_users_media_path)
        plt.close()
        report.add_image("Text vs. Media Messages for Top Active Users", f"./images/{os.path.basename(active_users_media_path)}")
    else:
        report.add_text("No data for active users' text vs. media messages.")

    # h.14: Os 30 usu√°rios que mais compartilharam texto;
    report.add_question("h.14", "Top 30 users who shared most text messages")
    top_text_sharers = df[df['has_media'] == False]['id_member_anonymous'].value_counts().head(30).reset_index()
    top_text_sharers.columns = ['User ID', 'Text Message Count']
    if not top_text_sharers.empty:
        report.add_table(top_text_sharers, title="Top 30 Text Sharers")
    else:
        report.add_text("No text sharers found.")

    # h.15: Os 30 usu√°rios que mais compartilharam m√≠dias;
    report.add_question("h.15", "Top 30 users who shared most media messages")
    top_media_sharers = df[df['has_media'] == True]['id_member_anonymous'].value_counts().head(30).reset_index()
    top_media_sharers.columns = ['User ID', 'Media Message Count']
    if not top_media_sharers.empty:
        report.add_table(top_media_sharers, title="Top 30 Media Sharers")
    else:
        report.add_text("No media sharers found.")

    # h.16: As 30 mensagens mais compartilhadas;
    report.add_question("h.16", "Top 30 most shared messages")
    top_shared_messages = df['text_content_anonymous'].value_counts().head(30).reset_index()
    top_shared_messages.columns = ['Message Text', 'Share Count']
    if not top_shared_messages.empty:
        report.add_table(top_shared_messages, title="Top 30 Shared Messages")
    else:
        report.add_text("No shared messages found.")

    # h.17: As 30 mensagens mais compartilhadas em grupos diferentes;
    report.add_question("h.17", "Top 30 messages shared in different groups")
    messages_in_diff_groups = df.groupby('text_content_anonymous')['id_group_anonymous'].nunique().sort_values(ascending=False).head(30).reset_index()
    messages_in_diff_groups.columns = ['Message Text', 'Unique Group Count']
    if not messages_in_diff_groups.empty:
        report.add_table(messages_in_diff_groups, title="Top 30 Messages in Different Groups")
    else:
        report.add_text("No messages shared in different groups found.")

    # h.18: Mensagens id√™nticas compartilhadas pelo mesmo usu√°rio (e suas quantidades);
    report.add_question("h.18", "Identical messages shared by the same user (and their quantities)")
    identical_messages_same_user = df.groupby(['id_member_anonymous', 'text_content_anonymous']).size().reset_index(name='count')
    identical_messages_same_user = identical_messages_same_user[identical_messages_same_user['count'] > 1].sort_values('count', ascending=False).head(30)
    if not identical_messages_same_user.empty:
        report.add_table(identical_messages_same_user, title="Top 30 Identical Messages by Same User")
    else:
        report.add_text("No identical messages shared by the same user found.")

    # h.19: Mensagens id√™nticas compartilhadas pelo mesmo usu√°rio em grupos distintos (e suas quantidades);
    report.add_question("h.19", "Identical messages shared by the same user in distinct groups (and their quantities)")
    identical_messages_user_multi_group = df.groupby(['id_member_anonymous', 'text_content_anonymous'])['id_group_anonymous'].nunique().reset_index(name='unique_group_count')
    identical_messages_user_multi_group = identical_messages_user_multi_group[identical_messages_user_multi_group['unique_group_count'] > 1].sort_values('unique_group_count', ascending=False).head(30)
    if not identical_messages_user_multi_group.empty:
        report.add_table(identical_messages_user_multi_group, title="Top 30 Identical Messages by Same User in Different Groups")
    else:
        report.add_text("No identical messages shared by the same user in distinct groups found.")

    # h.20: Os 30 unigramas, bigramas e trigramas mais compartilhados (ap√≥s a remo√ß√£o de stop words);
    report.add_question("h.20", "Top 30 unigrams, bigrams, and trigrams (after stop word removal)")
    # This requires NLTK or similar for proper stop word removal and n-gram generation.
    # For simplicity, I'll do a basic split and count for unigrams. Bigrams/trigrams are more complex.
    # A full implementation would involve:
    # 1. Download NLTK stop words: `import nltk; nltk.download('stopwords')`
    # 2. Tokenization and stop word removal
    # 3. N-gram generation
    
    # Basic Unigram count (without proper stop word removal)
    all_words = ' '.join(df['text_content_anonymous'].dropna().astype(str).str.lower()).split()
    unigram_counts = Counter(all_words).most_common(30)
    report.add_table(pd.DataFrame(unigram_counts, columns=['Unigram', 'Count']), title="Top 30 Unigrams (Basic)")
    report.add_text("Note: A more robust n-gram analysis requires proper stop word removal and tokenization, typically using libraries like NLTK.")

    # h.21: As 30 mensagens mais positivas (distintas);
    report.add_question("h.21", "Top 30 distinct positive messages")
    positive_messages = df[df['sentiment'] == 1]['text_content_anonymous'].drop_duplicates().head(30).to_frame()
    if not positive_messages.empty:
        report.add_table(positive_messages, title="Top 30 Distinct Positive Messages")
    else:
        report.add_text("No positive messages found.")

    # h.22: As 30 mensagens mais negativas (distintas);
    report.add_question("h.22", "Top 30 distinct negative messages")
    negative_messages = df[df['sentiment'] == -1]['text_content_anonymous'].drop_duplicates().head(30).to_frame()
    if not negative_messages.empty:
        report.add_table(negative_messages, title="Top 30 Distinct Negative Messages")
    else:
        report.add_text("No negative messages found.")

    # h.23: O usu√°rio mais otimista;
    report.add_question("h.23", "Most optimistic user")
    most_optimistic = df.groupby('id_member_anonymous')['sentiment'].sum().sort_values(ascending=False).head(1).reset_index()
    if not most_optimistic.empty:
        report.add_table(most_optimistic, title="Most Optimistic User")
    else:
        report.add_text("No data to determine most optimistic user.")

    # h.24: O usu√°rio mais pessimista;
    report.add_question("h.24", "Most pessimistic user")
    most_pessimistic = df.groupby('id_member_anonymous')['sentiment'].sum().sort_values(ascending=True).head(1).reset_index()
    if not most_pessimistic.empty:
        report.add_table(most_pessimistic, title="Most Pessimistic User")
    else:
        report.add_text("No data to determine most pessimistic user.")

    # h.25: As 30 maiores mensagens;
    report.add_question("h.25", "Top 30 longest messages")
    longest_messages = df[['text_content_anonymous', 'caracteres']].sort_values('caracteres', ascending=False).head(30)
    if not longest_messages.empty:
        report.add_table(longest_messages, title="Top 30 Longest Messages")
    else:
        report.add_text("No messages found.")

    # h.26: As 30 menores mensagens;
    report.add_question("h.26", "Top 30 shortest messages")
    shortest_messages = df[df['caracteres'] > 0][['text_content_anonymous', 'caracteres']].sort_values('caracteres', ascending=True).head(30)
    if not shortest_messages.empty:
        report.add_table(shortest_messages, title="Top 30 Shortest Messages")
    else:
        report.add_text("No messages found.")

    # h.27: O dia em que foi publicado a maior quantidade de mensagens;
    report.add_question("h.27", "Day with the highest quantity of messages")
    df['date_message'] = pd.to_datetime(df['date_message'])
    busiest_day = df['date_message'].dt.date.value_counts().head(1).reset_index()
    busiest_day.columns = ['Date', 'Message Count']
    if not busiest_day.empty:
        report.add_table(busiest_day, title="Busiest Day")
    else:
        report.add_text("No data to determine busiest day.")

    # h.28: As mensagens que possuem as palavras ‚ÄúFAC√á√ÉO‚Äù e ‚ÄúCRIMINOSA‚Äù;
    report.add_question("h.28", "Messages containing 'FAC√á√ÉO' and 'CRIMINOSA'")
    faccao_criminosa_messages = df[df['text_content_anonymous'].str.contains('FAC√á√ÉO', na=False, case=False) & df['text_content_anonymous'].str.contains('CRIMINOSA', na=False, case=False)]['text_content_anonymous'].head(30).to_frame()
    if not faccao_criminosa_messages.empty:
        report.add_table(faccao_criminosa_messages, title="Messages with 'FAC√á√ÉO' and 'CRIMINOSA'")
    else:
        report.add_text("No messages found containing both 'FAC√á√ÉO' and 'CRIMINOSA'.")

    # h.29: Quantidade de mensagens por dia e hora;
    report.add_question("h.29", "Quantity of messages by day and hour")
    df['date_hour'] = df['date_message'].dt.floor('H')
    messages_by_day_hour = df['date_hour'].value_counts().sort_index().reset_index()
    messages_by_day_hour.columns = ['Date_Hour', 'Message Count']
    
    plt.figure(figsize=(15, 7))
    sns.lineplot(x='Date_Hour', y='Message Count', data=messages_by_day_hour)
    plt.title('Message Count by Day and Hour')
    plt.xlabel('Date and Hour')
    plt.ylabel('Message Count')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    messages_by_day_hour_path = os.path.join(images_dir, 'h29_messages_by_day_hour.png')
    plt.savefig(messages_by_day_hour_path)
    plt.close()
    report.add_image("Message Count by Day and Hour", f"./images/{os.path.basename(messages_by_day_hour_path)}")

    # h.30: Quantidade de mensagens por hora (daily pattern);
    report.add_question("h.30", "Quantity of messages by hour (daily pattern)")
    df['hour_of_day'] = df['date_message'].dt.hour
    messages_by_hour = df['hour_of_day'].value_counts().sort_index().reset_index()
    messages_by_hour.columns = ['Hour', 'Message Count']
    
    plt.figure(figsize=(10, 6))
    sns.lineplot(x='Hour', y='Message Count', data=messages_by_hour)
    plt.title('Average Message Count by Hour of Day')
    plt.xlabel('Hour of Day')
    plt.ylabel('Message Count')
    plt.xticks(range(0, 24))
    plt.tight_layout()
    messages_by_hour_path = os.path.join(images_dir, 'h30_messages_by_hour.png')
    plt.savefig(messages_by_hour_path)
    plt.close()
    report.add_image("Average Message Count by Hour of Day", f"./images/{os.path.basename(messages_by_hour_path)}")

    # h.31: A nuvem de palavras referente √†s mensagens de texto (ap√≥s a remo√ß√£o de stop words);
    report.add_question("h.31", "Word cloud of text messages (after stop word removal)")
    report.add_text("Generating a proper word cloud requires `wordcloud` library and NLTK for stop words. This is a placeholder.")
    # Example placeholder image
    # report.add_image("Word Cloud", "./images/placeholder_wordcloud.png")

    # h.32: A rede interativa das palavras referente √†s mensagens de texto (ap√≥s a remo√ß√£o de stop words);
    report.add_question("h.32", "Interactive word network of text messages (after stop word removal)")
    report.add_text("Generating an interactive word network is complex and typically requires libraries like `networkx` and `bokeh` or `pyvis`. This is a placeholder.")
    # Example placeholder image
    # report.add_image("Word Network", "./images/placeholder_wordnetwork.png")

    # h.33: Propor√ß√£o de mensagens com e sem URL;
    report.add_question("h.33", "Proportion of messages with and without URL")
    url_proportion = df['has_media_url'].value_counts(normalize=True) * 100
    url_proportion.index = ['Without URL', 'With URL']
    plt.figure(figsize=(8, 8))
    plt.pie(url_proportion, labels=url_proportion.index, autopct='%1.1f%%', startangle=90)
    plt.title('Proportion of Messages With and Without URL')
    url_pie_path = os.path.join(images_dir, 'h33_url_proportion.png')
    plt.savefig(url_pie_path)
    plt.close()
    report.add_image("Proportion of Messages With and Without URL", f"./images/{os.path.basename(url_pie_path)}")

    # h.34: Propor√ß√£o de desinforma√ß√£o;
    report.add_question("h.34", "Proportion of misinformation")
    # Assuming score_misinformation > 0.5 is misinformation, < -0.5 is not, else neutral
    def classify_misinformation(score):
        if pd.isna(score): return 'Unknown'
        if score > 0.5: return 'Misinformation'
        if score < 0.5: return 'Not Misinformation' # Assuming 0.5 is the threshold
        return 'Neutral' # For scores around 0.5 or other cases
    
    df['misinformation_category'] = df['score_misinformation'].apply(classify_misinformation)
    misinfo_proportion = df['misinformation_category'].value_counts(normalize=True) * 100
    
    plt.figure(figsize=(8, 8))
    plt.pie(misinfo_proportion, labels=misinfo_proportion.index, autopct='%1.1f%%', startangle=90)
    plt.title('Proportion of Misinformation Categories')
    misinfo_pie_path = os.path.join(images_dir, 'h34_misinformation_proportion.png')
    plt.savefig(misinfo_pie_path)
    plt.close()
    report.add_image("Proportion of Misinformation Categories", f"./images/{os.path.basename(misinfo_pie_path)}")

    # h.35: Propor√ß√£o de mensagens contendo m√≠dia e desinforma√ß√£o;
    report.add_question("h.35", "Proportion of messages containing media and misinformation")
    media_misinfo_counts = df.groupby(['has_media', 'misinformation_category']).size().unstack(fill_value=0)
    
    # This plot might be complex. A stacked bar chart or a grouped bar chart is better.
    # For simplicity, let's show a table and a basic plot if possible.
    report.add_table(media_misinfo_counts, title="Media vs. Misinformation Counts")

    # h.36: Distribui√ß√£o de mensagens por score de desinforma√ß√£o;
    report.add_question("h.36", "Distribution of messages by misinformation score")
    plt.figure(figsize=(10, 6))
    sns.histplot(df['score_misinformation'].dropna(), bins=30, kde=True)
    plt.title('Distribution of Misinformation Score')
    misinfo_score_dist_path = os.path.join(images_dir, 'h36_misinfo_score_distribution.png')
    plt.savefig(misinfo_score_dist_path)
    plt.close()
    report.add_image("Distribution of Misinformation Score", f"./images/{os.path.basename(misinfo_score_dist_path)}")

    # h.37: Propor√ß√£o de sentimentos;
    report.add_question("h.37", "Proportion of sentiments")
    sentiment_proportion = df['sentiment'].value_counts(normalize=True) * 100
    sentiment_proportion.index = sentiment_proportion.index.map({1: 'Positive', 0: 'Neutral', -1: 'Negative'})
    plt.figure(figsize=(8, 8))
    plt.pie(sentiment_proportion, labels=sentiment_proportion.index, autopct='%1.1f%%', startangle=90)
    plt.title('Proportion of Sentiments')
    sentiment_pie_path = os.path.join(images_dir, 'h37_sentiment_proportion.png')
    plt.savefig(sentiment_pie_path)
    plt.close()
    report.add_image("Proportion of Sentiments", f"./images/{os.path.basename(sentiment_pie_path)}")

    # h.38: Distribui√ß√£o de mensagens por score de sentimentos;
    report.add_question("h.38", "Distribution of messages by sentiment score")
    plt.figure(figsize=(10, 6))
    sns.histplot(df['score_sentiment'].dropna(), bins=30, kde=True)
    plt.title('Distribution of Sentiment Score')
    sentiment_score_dist_path = os.path.join(images_dir, 'h38_sentiment_score_distribution.png')
    plt.savefig(sentiment_score_dist_path)
    plt.close()
    report.add_image("Distribution of Sentiment Score", f"./images/{os.path.basename(sentiment_score_dist_path)}")

    # h.39: Propor√ß√£o entre mensagens virais e n√£o virais;
    report.add_question("h.39", "Proportion of viral vs. non-viral messages")
    viral_proportion = df['viral'].value_counts(normalize=True) * 100
    viral_proportion.index = ['Non-Viral', 'Viral']
    plt.figure(figsize=(8, 8))
    plt.pie(viral_proportion, labels=viral_proportion.index, autopct='%1.1f%%', startangle=90)
    plt.title('Proportion of Viral vs. Non-Viral Messages')
    viral_pie_path = os.path.join(images_dir, 'h39_viral_proportion.png')
    plt.savefig(viral_pie_path)
    plt.close()
    report.add_image("Proportion of Viral vs. Non-Viral Messages", f"./images/{os.path.basename(viral_pie_path)}")

    # h.40: Algo que voc√™ julga importante e que ainda n√£o foi solicitado;
    report.add_question("h.40", "Additional important insights not explicitly requested")
    report.add_text("One important aspect not explicitly requested is the **temporal trend of misinformation**. Analyzing how misinformation scores change over time could reveal patterns related to events or campaigns. Another is **network analysis of user interactions** (if interaction data were available), which could identify influential users or communities spreading misinformation.")
    
    # --- Save the Final Report ---
    report_path = os.path.join(project_root, 'prj_part03', 'report.md')
    report.save_report(report_path)

if __name__ == '__main__':
    main()

